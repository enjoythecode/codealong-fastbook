{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Practice\n",
    "# Classify between 3s and 8s in MNIST\n",
    "# Can only look at fast.ai notebook for how the data is originally loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastbook as fb\n",
    "??fb.URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb.URLs.MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('/Users/sinany/.fastai/data/mnist_png/training'),Path('/Users/sinany/.fastai/data/mnist_png/testing')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = fb.untar_data(fb.URLs.MNIST)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_0s = fb.torch.stack([fb.tensor(fb.Image.open(img)) for img in (path/\"training\"/'0').ls()]).float()/255\n",
    "train_8s = fb.torch.stack([fb.tensor(fb.Image.open(img)) for img in (path/\"training\"/'8').ls()]).float()/255\n",
    "\n",
    "test_0s = fb.torch.stack([fb.tensor(fb.Image.open(img)) for img in (path/\"testing\"/'0').ls()]).float()/255\n",
    "test_8s = fb.torch.stack([fb.tensor(fb.Image.open(img)) for img in (path/\"testing\"/'8').ls()]).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model: compare distance to \"mean\" 0 or 8\n",
    "\n",
    "# calculate the mean digits\n",
    "mean_0 = train_0s.mean(0) # take the mean across images (the first dimension)\n",
    "mean_8 = train_8s.mean(0)\n",
    "\n",
    "# classify as '0' for 0, '1' for 8\n",
    "def baseline(x):\n",
    "    return ((mean_0 - x).abs().mean((1,2)) > (mean_8 - x).abs().mean((1,2))).float()\n",
    "\n",
    "y_base0 = baseline(test_0s)\n",
    "y_base8 = baseline(test_8s)\n",
    "\n",
    "print(f\"baseline accurracy for 0s is {1 - y_base0.sum() / y_base0.size()[0]}\")\n",
    "print(f\"baseline accurracy for 8s is {y_base8.sum() / y_base8.size()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural model: learn weights for a two layer model that is coded from scratch (everything except gradients)\n",
    "\n",
    "# this model is not supposed to be elegant, extensible, or efficient. it is meant to be crafted from memory\n",
    "# this is an exercise in first-principles understanding of machine learning, not engineering\n",
    "\n",
    "mid_size = 256 # how many hidden neurons?\n",
    "\n",
    "# randomly initialize our parameters\n",
    "params = [\n",
    "    fb.torch.randn((28 * 28, mid_size)), # weights for layer 0\n",
    "    fb.torch.randn((mid_size)), # biases for layer 0\n",
    "    fb.torch.randn((mid_size, 1)), # weights for layer 1\n",
    "    fb.torch.randn(1), # bias for layer 1\n",
    "]\n",
    "\n",
    "# start tracking the gradient\n",
    "for i in range(len(params)):\n",
    "    params[i].requires_grad_()\n",
    "\n",
    "def model(x):\n",
    "    \n",
    "    x = x.flatten(1)\n",
    "    \n",
    "    # layer 0\n",
    "    activations = x @ params[0]\n",
    "    activations += params[1]\n",
    "    \n",
    "    # non-linearity\n",
    "    activations = fb.torch.maximum(activations,fb.tensor(0)) # ReLU\n",
    "    \n",
    "    # layer 1\n",
    "    activations = activations @ params[2]\n",
    "    activations += params[3]\n",
    "    \n",
    "    return activations # return a probability in [0, 1] of how much the model thinks this is an 8\n",
    "\n",
    "def accuracy():\n",
    "    y_nn0 = model_pred(train_0s)\n",
    "    y_nn8 = model_pred(train_8s)\n",
    "    \n",
    "    acc_0 = 1 - y_nn0.sum() / y_nn0.size()[0]\n",
    "    acc_8 = y_nn8.sum() / y_nn8.size()[0]\n",
    "    acc = ((y_nn0.size()[0] - y_nn0.sum()) + y_nn8.sum()) / (y_nn0.size()[0] + y_nn8.size()[0])\n",
    "    print(acc.item(), acc_0.item(), acc_8.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5901987552642822 0.9800776839256287 0.19552212953567505\n",
      "0.5927467346191406 0.9807530045509338 0.19996581971645355\n",
      "0.6065908074378967 0.9760256409645081 0.2326098084449768\n",
      "0.6082045435905457 0.9775451421737671 0.23431892693042755\n",
      "0.6155087351799011 0.9768698215484619 0.2497009038925171\n",
      "0.6144046187400818 0.978220522403717 0.24611178040504456\n",
      "0.631136417388916 0.9733242988586426 0.28473764657974243\n",
      "0.6316459774971008 0.9753503203392029 0.2837121784687042\n",
      "0.6462544798851013 0.9724801778793335 0.3160143494606018\n",
      "0.6385255455970764 0.9755191802978516 0.2973850667476654\n",
      "0.6258705854415894 0.9787269830703735 0.26867201924324036\n",
      "0.6334295868873596 0.9777140021324158 0.28490856289863586\n",
      "0.6595889329910278 0.9731554985046387 0.34216374158859253\n",
      "0.6874468922615051 0.9631943106651306 0.4083062708377838\n",
      "0.7100390791893005 0.9552591443061829 0.4618014097213745\n",
      "0.7080855965614319 0.958129346370697 0.4549649655818939\n",
      "0.7105486392974854 0.9593111872673035 0.4587250053882599\n",
      "0.6991676688194275 0.96673983335495 0.4283028542995453\n",
      "0.7286393642425537 0.9522201418876648 0.5023072957992554\n",
      "0.728044867515564 0.9572851657867432 0.4959836006164551\n",
      "0.7620180249214172 0.9377005100250244 0.5841736197471619\n",
      "0.7618481516838074 0.9451291561126709 0.5763117671012878\n",
      "0.7872430682182312 0.9366874694824219 0.6359596848487854\n",
      "0.7957363724708557 0.9302718043327332 0.659545361995697\n",
      "0.8059282898902893 0.9132196307182312 0.6973167061805725\n",
      "0.7945473194122314 0.9292588233947754 0.6581780910491943\n",
      "0.8080516457557678 0.9233496785163879 0.6913348436355591\n",
      "0.8299643397331238 0.8997129797935486 0.7593573927879333\n",
      "0.8461865186691284 0.8743879795074463 0.8176380395889282\n",
      "0.8550195097923279 0.8608813285827637 0.8490856289863586\n",
      "0.8580771088600159 0.8819854855537415 0.833874523639679\n",
      "0.8601155281066895 0.8916089534759521 0.8282344937324524\n",
      "0.8680142760276794 0.8576734662055969 0.8784822821617126\n",
      "0.8651265501976013 0.8347121477127075 0.8959152102470398\n",
      "0.870392382144928 0.8470369577407837 0.8940352201461792\n",
      "0.8761678338050842 0.869154155254364 0.8832678198814392\n",
      "0.8801596760749817 0.8816478252410889 0.8786532282829285\n",
      "0.8816035389900208 0.8882322907447815 0.8748931884765625\n",
      "0.8770171403884888 0.8617254495620728 0.8924970030784607\n",
      "0.8817734122276306 0.8899206519126892 0.8735259175300598\n",
      "0.8808391094207764 0.9133884906768799 0.847889244556427\n",
      "0.8821980357170105 0.9100118279457092 0.8540420532226562\n",
      "0.8853405714035034 0.9057909846305847 0.8646385073661804\n",
      "0.8913708329200745 0.9086611270904541 0.8738676905632019\n",
      "0.8945982456207275 0.9057909846305847 0.8832678198814392\n",
      "0.8959571719169617 0.9083234667778015 0.8834387063980103\n",
      "0.892220139503479 0.9312848448753357 0.8526747822761536\n",
      "0.8940886855125427 0.9358433485031128 0.8518202304840088\n",
      "0.8941736221313477 0.9390511512756348 0.8487437963485718\n",
      "0.8994394540786743 0.9370251893997192 0.8613911867141724\n",
      "0.903176486492157 0.9333108067512512 0.872671365737915\n",
      "0.9069984555244446 0.9297653436660767 0.8839514851570129\n",
      "0.9064888954162598 0.9334796667098999 0.8791659474372864\n",
      "0.9090368747711182 0.9333108067512512 0.8844642043113708\n",
      "0.9069984555244446 0.9436096549034119 0.8699367642402649\n",
      "0.9064039587974548 0.948168158531189 0.8641257882118225\n",
      "0.906234085559845 0.9498565196990967 0.8620748519897461\n",
      "0.9058943390846252 0.9518824815750122 0.8593403100967407\n",
      "0.9148972034454346 0.9339861869812012 0.8955733776092529\n",
      "0.9160863161087036 0.9351679682731628 0.8967697620391846\n",
      "0.9134534001350403 0.9471551775932312 0.8793368935585022\n",
      "0.9122642874717712 0.9498565196990967 0.8742095232009888\n",
      "0.909206748008728 0.9527266621589661 0.8651512265205383\n",
      "0.910735547542572 0.9528955221176147 0.8680567145347595\n",
      "0.9086971282958984 0.9589734673500061 0.8578020930290222\n",
      "0.910735547542572 0.9574539661407471 0.8634421229362488\n",
      "0.9124341607093811 0.9535708427429199 0.8707913160324097\n",
      "0.9099711179733276 0.9604929685592651 0.858827531337738\n",
      "0.9114149808883667 0.9603241682052612 0.861903965473175\n",
      "0.9129437804222107 0.9593111872673035 0.8660058379173279\n",
      "0.8987599611282349 0.9706230163574219 0.8260126709938049\n",
      "0.9011381268501282 0.9702853560447693 0.8311399817466736\n",
      "0.9057244658470154 0.9680904746055603 0.8425909876823425\n",
      "0.8977407813072205 0.9718048572540283 0.8227653503417969\n",
      "0.9132835268974304 0.9655579924583435 0.8603657484054565\n",
      "0.9183794856071472 0.9638696908950806 0.8723295331001282\n",
      "0.9160863161087036 0.9669086337089539 0.8646385073661804\n",
      "0.9166808128356934 0.9669086337089539 0.8658348917961121\n",
      "0.9192287921905518 0.9658956527709961 0.8719877004623413\n",
      "0.920502781867981 0.9652203321456909 0.8752350211143494\n",
      "0.9216069579124451 0.9643761515617371 0.8783113956451416\n",
      "0.9210973381996155 0.96673983335495 0.8748931884765625\n",
      "0.9197384119033813 0.9691035151481628 0.8697658777236938\n",
      "0.9177849292755127 0.9704541563987732 0.8644676208496094\n",
      "0.9212672114372253 0.9689346551895142 0.8730131387710571\n",
      "0.9225412011146545 0.968259334564209 0.8762604594230652\n",
      "0.917105495929718 0.9718048572540283 0.8617330193519592\n",
      "0.9194836020469666 0.9706230163574219 0.8677149415016174\n",
      "0.9183794856071472 0.9706230163574219 0.8654930591583252\n",
      "0.9225412011146545 0.9679216742515564 0.876602292060852\n",
      "0.9262782335281372 0.96673983335495 0.8853187561035156\n",
      "0.9213521480560303 0.9699476361274719 0.8721585869789124\n",
      "0.9250891804695129 0.9670774936676025 0.8825841546058655\n",
      "0.9165958762168884 0.9733242988586426 0.8591693639755249\n",
      "0.9239850640296936 0.968259334564209 0.8791659474372864\n",
      "0.9305248856544495 0.9631943106651306 0.8974534273147583\n",
      "0.9272124767303467 0.9675840139389038 0.8863441944122314\n",
      "0.9230507612228394 0.9709606766700745 0.8745513558387756\n",
      "0.9221165180206299 0.9721425175666809 0.8714749813079834\n",
      "0.9185493588447571 0.9731554985046387 0.8632712364196777\n",
      "0.9205877184867859 0.9733242988586426 0.8672021627426147\n",
      "0.9138780236244202 0.9770386815071106 0.8499401807785034\n",
      "0.9252590537071228 0.971298336982727 0.8786532282829285\n",
      "0.9068285822868347 0.9795711636543274 0.83319091796875\n",
      "0.9148972034454346 0.9763633012771606 0.8526747822761536\n",
      "0.9194836020469666 0.9739996790885925 0.8642966747283936\n",
      "0.9209274649620056 0.9734931588172913 0.8677149415016174\n",
      "0.9259384870529175 0.971298336982727 0.8800204992294312\n",
      "0.923560380935669 0.9721425175666809 0.8743804693222046\n",
      "0.9229658842086792 0.9723113179206848 0.8730131387710571\n",
      "0.9244096875190735 0.9716359972953796 0.876602292060852\n",
      "0.9315440654754639 0.9655579924583435 0.8971115946769714\n",
      "0.9270426630973816 0.9707918167114258 0.8827551007270813\n",
      "0.9336674213409424 0.9643761515617371 0.902580738067627\n",
      "0.9388483166694641 0.9571163058280945 0.920355498790741\n",
      "0.9379140734672546 0.9603241682052612 0.9152281880378723\n",
      "0.9383386969566345 0.9594799876213074 0.9169372916221619\n",
      "0.9396126866340637 0.9562721848487854 0.9227482676506042\n",
      "0.9388483166694641 0.9564409852027893 0.9210391640663147\n",
      "0.9399524331092834 0.9571163058280945 0.9225773215293884\n",
      "0.9395277500152588 0.9616748094558716 0.9171081781387329\n",
      "0.9346016645431519 0.9679216742515564 0.9008716344833374\n",
      "0.9362154006958008 0.9680904746055603 0.9039480686187744\n",
      "0.9384236335754395 0.9672463536262512 0.9092462658882141\n",
      "0.9406319260597229 0.9652203321456909 0.9157409071922302\n",
      "0.9436894655227661 0.9594799876213074 0.9277046918869019\n",
      "0.9424154758453369 0.9604929685592651 0.9241155385971069\n",
      "0.9410565495491028 0.9640384912490845 0.9177918434143066\n",
      "0.9348564743995667 0.9718048572540283 0.8974534273147583\n",
      "0.9320536851882935 0.9739996790885925 0.8895915150642395\n",
      "0.9251741170883179 0.9773763418197632 0.8723295331001282\n",
      "0.9261083602905273 0.9777140021324158 0.8738676905632019\n",
      "0.9306098222732544 0.9761945009231567 0.8844642043113708\n",
      "0.9396126866340637 0.971298336982727 0.9075371623039246\n",
      "0.9354509711265564 0.9750126600265503 0.8954024910926819\n",
      "0.931119441986084 0.976701021194458 0.8849769234657288\n",
      "0.9378291368484497 0.975181519985199 0.9000170826911926\n",
      "0.938933253288269 0.9748438596725464 0.902580738067627\n",
      "0.943859338760376 0.9675840139389038 0.9198427796363831\n",
      "0.9463223814964294 0.9642073512077332 0.9282174110412598\n",
      "0.9496347904205322 0.958129346370697 0.9410357475280762\n",
      "0.9485306739807129 0.9599865078926086 0.9369338750839233\n",
      "0.9477662444114685 0.9647138118743896 0.930610179901123\n",
      "0.9477662444114685 0.9652203321456909 0.9300974011421204\n",
      "0.948360800743103 0.9638696908950806 0.9326610565185547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9482758641242981 0.9643761515617371 0.9319774508476257\n",
      "0.9501444101333618 0.9588046669960022 0.9413775205612183\n",
      "0.949889600276947 0.9616748094558716 0.9379593133926392\n",
      "0.9513334631919861 0.958129346370697 0.9444539546966553\n",
      "0.9492101073265076 0.9638696908950806 0.934370219707489\n",
      "0.9492950439453125 0.9648826718330383 0.9335156679153442\n",
      "0.946832001209259 0.9684281349182129 0.9249700903892517\n",
      "0.9511635899543762 0.9618436694145203 0.9403520822525024\n",
      "0.9503991603851318 0.9633631706237793 0.9372757077217102\n",
      "0.9527773261070251 0.9603241682052612 0.9451375603675842\n",
      "0.9526074528694153 0.9625189900398254 0.9425739049911499\n",
      "0.949974536895752 0.9684281349182129 0.931293785572052\n",
      "0.9490402340888977 0.9716359972953796 0.9261664748191833\n",
      "0.9456429481506348 0.9760256409645081 0.9148863554000854\n",
      "0.9455580115318298 0.9758568406105042 0.9148863554000854\n",
      "0.9467470645904541 0.9748438596725464 0.9183045625686646\n",
      "0.9464073181152344 0.9755191802978516 0.9169372916221619\n",
      "0.9463223814964294 0.9758568406105042 0.916424572467804\n",
      "0.9440292119979858 0.976701021194458 0.9109553694725037\n",
      "0.9450483918190002 0.9765321612358093 0.9131772518157959\n",
      "0.9396126866340637 0.9790646433830261 0.8996752500534058\n",
      "0.9424154758453369 0.9777140021324158 0.9066826105117798\n",
      "0.9418209791183472 0.9785581827163696 0.9046316742897034\n",
      "0.9439442753791809 0.9773763418197632 0.9101008176803589\n",
      "0.9421606659889221 0.9780516624450684 0.905828058719635\n",
      "0.9454730749130249 0.9761945009231567 0.9143736362457275\n",
      "0.942245602607727 0.9778828024864197 0.9061698913574219\n",
      "0.9417360424995422 0.9788958430290222 0.9041189551353455\n",
      "0.9419059157371521 0.9788958430290222 0.9044607877731323\n",
      "0.9395277500152588 0.9797400236129761 0.898820698261261\n",
      "0.9377442002296448 0.9805841445922852 0.8943770527839661\n",
      "0.9401223063468933 0.9797400236129761 0.9000170826911926\n",
      "0.9383386969566345 0.9802464842796326 0.8959152102470398\n",
      "0.9391031265258789 0.97990882396698 0.8977952599525452\n",
      "0.9384236335754395 0.9807530045509338 0.8955733776092529\n",
      "0.942075788974762 0.9797400236129761 0.9039480686187744\n",
      "0.9485306739807129 0.9760256409645081 0.9206973314285278\n",
      "0.9485306739807129 0.9761945009231567 0.920526385307312\n",
      "0.9503142237663269 0.9733242988586426 0.9270210266113281\n",
      "0.9512485265731812 0.9726489782333374 0.9295846819877625\n",
      "0.9527773261070251 0.9716359972953796 0.9336865544319153\n",
      "0.9522677063941956 0.9719736576080322 0.9323192834854126\n",
      "0.9535416960716248 0.9707918167114258 0.9360793232917786\n",
      "0.9508238434791565 0.9753503203392029 0.9259955286979675\n",
      "0.951418399810791 0.9756879806518555 0.9268501400947571\n",
      "0.951758086681366 0.975181519985199 0.928046464920044\n",
      "0.9507389068603516 0.9763633012771606 0.9247992038726807\n",
      "0.9476813077926636 0.9790646433830261 0.9159117937088013\n",
      "0.9506539702415466 0.9761945009231567 0.9247992038726807\n",
      "0.9520128965377808 0.9748438596725464 0.9289010167121887\n",
      "0.9545608758926392 0.9711294770240784 0.9377884268760681\n",
      "0.9550704956054688 0.9706230163574219 0.9393265843391418\n",
      "0.9535416960716248 0.9741684794425964 0.9326610565185547\n",
      "0.9536266326904297 0.974506139755249 0.9324901700019836\n",
      "0.9560896754264832 0.9709606766700745 0.9410357475280762\n",
      "0.956259548664093 0.9691035151481628 0.9432575702667236\n",
      "0.9556650519371033 0.9716359972953796 0.9394975304603577\n",
      "0.9554951786994934 0.971298336982727 0.9394975304603577\n",
      "0.9538814425468445 0.9746749997138977 0.9328320026397705\n",
      "0.9520128965377808 0.9768698215484619 0.9268501400947571\n",
      "0.9471717476844788 0.9795711636543274 0.9143736362457275\n",
      "0.9515882730484009 0.978220522403717 0.9246282577514648\n",
      "0.9512485265731812 0.9780516624450684 0.9241155385971069\n",
      "0.9558348655700684 0.9739996790885925 0.9374465942382812\n",
      "0.9541362524032593 0.9755191802978516 0.9324901700019836\n",
      "0.9535416960716248 0.9760256409645081 0.9307810664176941\n",
      "0.9559198021888733 0.974506139755249 0.9371047616004944\n",
      "0.9565992951393127 0.9726489782333374 0.9403520822525024\n",
      "0.9541362524032593 0.9763633012771606 0.9316356182098389\n",
      "0.953201949596405 0.9773763418197632 0.9287301301956177\n",
      "0.954730749130249 0.9763633012771606 0.9328320026397705\n",
      "0.9537965059280396 0.9778828024864197 0.9294137954711914\n",
      "0.9552403688430786 0.9760256409645081 0.9341992735862732\n",
      "0.9549855589866638 0.9763633012771606 0.9333447217941284\n",
      "0.954815685749054 0.9763633012771606 0.9330028891563416\n",
      "0.9544759392738342 0.9773763418197632 0.931293785572052\n",
      "0.9546458125114441 0.9763633012771606 0.9326610565185547\n",
      "0.9550704956054688 0.9763633012771606 0.9335156679153442\n",
      "0.9551554322242737 0.9772074818611145 0.9328320026397705\n",
      "0.9542211890220642 0.9785581827163696 0.9295846819877625\n",
      "0.9502293467521667 0.97990882396698 0.9201846122741699\n",
      "0.9554951786994934 0.9777140021324158 0.9330028891563416\n",
      "0.9546458125114441 0.978389322757721 0.930610179901123\n",
      "0.9565992951393127 0.9765321612358093 0.9364210963249207\n",
      "0.957788348197937 0.9756879806518555 0.9396684169769287\n",
      "0.9582979679107666 0.9756879806518555 0.9406939148902893\n",
      "0.9549855589866638 0.9787269830703735 0.9309519529342651\n",
      "0.9523526430130005 0.9802464842796326 0.9241155385971069\n",
      "0.9543910026550293 0.9790646433830261 0.9294137954711914\n",
      "0.9544759392738342 0.97990882396698 0.9287301301956177\n",
      "0.9490402340888977 0.9827790260314941 0.9148863554000854\n",
      "0.9482758641242981 0.9832854866981506 0.912835419178009\n",
      "0.9506539702415466 0.9812595248222351 0.9196718335151672\n",
      "0.9544759392738342 0.97990882396698 0.9287301301956177\n",
      "0.9531170129776001 0.9807530045509338 0.9251409769058228\n",
      "0.9564294219017029 0.9792335033416748 0.9333447217941284\n",
      "0.9558348655700684 0.9794023036956787 0.9319774508476257\n",
      "0.9501444101333618 0.9815971851348877 0.9183045625686646\n",
      "0.9543910026550293 0.9802464842796326 0.9282174110412598\n",
      "0.9564294219017029 0.9785581827163696 0.9340283870697021\n",
      "0.9594870209693909 0.9738308191299438 0.9449666738510132\n",
      "0.9583829045295715 0.9760256409645081 0.9405229687690735\n",
      "0.9591472744941711 0.9743373394012451 0.9437702894210815\n",
      "0.9576184749603271 0.9777140021324158 0.9372757077217102\n",
      "0.9574486017227173 0.9780516624450684 0.9365920424461365\n",
      "0.9581280946731567 0.978389322757721 0.9376174807548523\n",
      "0.9567691683769226 0.9794023036956787 0.9338574409484863\n",
      "0.9564294219017029 0.9797400236129761 0.9328320026397705\n",
      "0.9571088552474976 0.9792335033416748 0.9347119927406311\n",
      "0.9570239782333374 0.9792335033416748 0.9345411062240601\n",
      "0.9553253054618835 0.9797400236129761 0.930610179901123\n",
      "0.9570239782333374 0.9794023036956787 0.934370219707489\n",
      "0.956259548664093 0.9797400236129761 0.9324901700019836\n",
      "0.9573636651039124 0.9790646433830261 0.9353956580162048\n",
      "0.9578732848167419 0.9787269830703735 0.9367629289627075\n",
      "0.9585527181625366 0.9785581827163696 0.938301146030426\n",
      "0.9579582214355469 0.9788958430290222 0.9367629289627075\n",
      "0.9576184749603271 0.9794023036956787 0.9355665445327759\n",
      "0.9578732848167419 0.9785581827163696 0.9369338750839233\n",
      "0.9606760740280151 0.9734931588172913 0.9477012753486633\n",
      "0.9591472744941711 0.9780516624450684 0.9400102496147156\n",
      "0.9569390416145325 0.9795711636543274 0.9340283870697021\n",
      "0.9571088552474976 0.9797400236129761 0.9341992735862732\n",
      "0.959317147731781 0.9788958430290222 0.9394975304603577\n",
      "0.9622048735618591 0.9723113179206848 0.9519740343093872\n",
      "0.9611856341362 0.9755191802978516 0.9466757774353027\n",
      "0.9617801904678345 0.9734931588172913 0.9499230980873108\n",
      "0.9620350003242493 0.9755191802978516 0.9483848810195923\n",
      "0.9611007571220398 0.9773763418197632 0.9446248412132263\n",
      "0.9582979679107666 0.9795711636543274 0.9367629289627075\n"
     ]
    }
   ],
   "source": [
    "# our NN has random performance: ~50% most of the time\n",
    "    \n",
    "# define loss function\n",
    "def loss(preds, truths):\n",
    "    preds = preds.sigmoid()\n",
    "    return fb.torch.where(truths==1, 1-preds, preds).mean()\n",
    "\n",
    "# create out training dataset\n",
    "train_x = fb.torch.cat([train_0s, train_8s]).view(-1,28,28)\n",
    "train_y = fb.tensor([0] * len(train_0s) + [1] * len(train_8s))\n",
    "\n",
    "NUM_EPOCHS = 1000\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_R = 0.1\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    indices = fb.torch.randperm(len(train_x))\n",
    "    for i in range(0, len(train_x), BATCH_SIZE):\n",
    "        batch_x = train_x[indices[i:i + BATCH_SIZE]]\n",
    "        batch_y = train_y[indices[i:i + BATCH_SIZE]]\n",
    "        \n",
    "        preds = model(batch_x)\n",
    "        lossv = loss(preds, batch_y)\n",
    "        lossv.backward()\n",
    "        for param in params:\n",
    "            param.data -= param.grad * LEARNING_R\n",
    "            param.grad.zero_()\n",
    "            \n",
    "    accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
