{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinan Yumurtaci\n",
    "# Code along for Chapter 4 of fast.ai's Practical Deep Learning for Coders\n",
    "\n",
    "# Digit classification task\n",
    "# Distinguish between images of digits\n",
    "# Data source: MNIST\n",
    "\n",
    "# 2nd section dedicated for using a NN on MNIST\n",
    "\n",
    "# 2021-03-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports as we follow along with fastbook\n",
    "\n",
    "from fastbook import *\n",
    "setup_book()\n",
    "from fastai.vision.widgets import *\n",
    "\n",
    "matplotlib.rc(\"image\", cmap = \"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6131 3s and 5421 5s.\n",
      "Loaded 6131 3s and 5421 5s!\n"
     ]
    }
   ],
   "source": [
    "# load the MNIST dataset\n",
    "# book loads only 3 and 7 for demonstration.\n",
    "\n",
    "# I'll be working on the classification for 3s and 5s\n",
    "# so that I learn new stuff!\n",
    "path = untar_data(URLs.MNIST)\n",
    "threes = (path/\"training\"/\"3\").ls().sorted()\n",
    "fives = (path/\"training\"/\"5\").ls().sorted()\n",
    "print(f\"There are {len(threes)} 3s and {len(fives)} 5s.\")\n",
    "\n",
    "# load the actual images into tensors\n",
    "three_tensors = [tensor(Image.open(f)) for f in threes]\n",
    "five_tensors = [tensor(Image.open(f)) for f in fives]\n",
    "\n",
    "print(f\"Loaded {len(three_tensors)} 3s and {len(five_tensors)} 5s!\")\n",
    "\n",
    "# stack the images for faster computation with PyTorch\n",
    "# also convert from [0, 255] to [0, 1]\n",
    "stacked_threes = torch.stack(three_tensors).float()/255\n",
    "stacked_fives = torch.stack(five_tensors).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put our training data in one variable\n",
    "train_x = torch.cat([stacked_threes, stacked_fives]).view(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11552, 784]), torch.Size([11552, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor of labels for our training data\n",
    "train_y = tensor([1]*len(threes) + [0] * len(fives)).unsqueeze(1)\n",
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip our data and labels into a dataset (a tuple of (x,y), as PyTorch requires)\n",
    "dset = list(zip(train_x, train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11552, 1902)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat the above steps for our test set\n",
    "test_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'testing'/'3').ls()]).float()/255\n",
    "test_5_tens = torch.stack([tensor(Image.open(o)) for o in (path/'testing'/'5').ls()]).float()/255\n",
    "\n",
    "test_x = torch.cat([test_3_tens, test_5_tens]).view(-1, 28*28)\n",
    "test_y = tensor([1]*len(test_3_tens) + [0] * len(test_5_tens)).unsqueeze(1)\n",
    "test_dset = list(zip(test_x, test_y))\n",
    "\n",
    "len(dset), len(test_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function that will return randomly initialized parameters of the specified size\n",
    "def init_params(size, std = 1.0):\n",
    "    return (torch.randn(size) * std).requires_grad_()\n",
    "\n",
    "weights = init_params((28*28,1))\n",
    "bias    = init_params(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.3087],\n",
       "        [-3.4366],\n",
       "        [ 0.8637],\n",
       "        ...,\n",
       "        [ 7.1891],\n",
       "        [ 9.6996],\n",
       "        [16.6568]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create function that will calculate prediction using the input, and the weights and the bias\n",
    "def linear_calc(xb):\n",
    "    return xb @ weights + bias\n",
    "\n",
    "# calculate predictions for the training set using our initial, random parameters\n",
    "preds = linear_calc(train_x)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36088988184928894"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate our accuracy on the training set\n",
    "corrects = (preds > 0.5).float() == train_y # if prediction is stronger than 0.5, count as predicting for 3. else, 5\n",
    "corrects.float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Loss Function\n",
    "Simply accuracy won't work as a loss function, because it is not continious. If we were to change one of the parameters slightly, the accuracy would not change, as demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36088988184928894"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0] *= 1.00001\n",
    "preds = linear_calc(train_x)\n",
    "((preds>0.5).float() == train_y).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the Problem?\n",
    "The accuracy is the same after the small change, so our model won't have any way of knowing which small steps to take to better fit our training data. We need another metric that is continious and responds to small changes like above, so that we can deduce the next small step to take in the optimization loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a loss function for this specific MNIST task of distinguishing between 3s and 5s\n",
    "def mnist_loss(predictions, targets):\n",
    "    predictions = predictions.sigmoid() # sigmoid ensures that all values are between 0 and 1\n",
    "    # so that we can use the mean of distance from the target in the standardized units of 0 and 1.\n",
    "    return torch.where(targets == 1, 1-predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Note: Metrics vs Loss Functions\n",
    "Metrics is what we care about as humans. This is usually some form of accuracy; we care about how accurate models are.\n",
    "\n",
    "The machine learning model does not directly optimize for the metric. It insteads minimizes its loss function. This is easier because loss functions are designed to have meaningful gradients.\n",
    "\n",
    "Improving the loss function also tends to improve the metric simply because we design loss functions specifically for that use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "optimization_step:\n",
    "\n",
    "    pred = model(x)\n",
    "    loss = loss_func(pred, x)\n",
    "    loss.backward()\n",
    "    params -= parameters.grad * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our parameters\n",
    "weights = init_params((28*28, 1))\n",
    "bias = init_params(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataLoader from our dataset (zip of inputs and targets)\n",
    "dl = DataLoader(dset, batch_size=256)\n",
    "xb,yb = first(dl)\n",
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(test_dset, batch_size=256) # same for our testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sub-function that will do a forward and backward pass\n",
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    loss.backward()\n",
    "\n",
    "# function for one epoch (iteration over the dataset)\n",
    "def train_epoch(model, lr, params):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad*lr # using p.data rather than p prevents the calculation of the graident for this step\n",
    "            p.grad.zero_() # reset the gradient after each mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6444"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create functions for tracking the accuracy of our model\n",
    "def batch_accuracy(xb, yb):\n",
    "    preds = xb.sigmoid()\n",
    "    correct = (preds>0.5) == yb\n",
    "    return correct.float().mean()\n",
    "\n",
    "def epoch_accuracy(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb,yb in test_dl]\n",
    "    return round(torch.stack(accs).mean().item(), 4)\n",
    "\n",
    "epoch_accuracy(linear_calc) # this is our starting accuracy with random parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9036 0.9061 0.908 0.911 0.9124 0.9139 0.9154 0.9173 0.9173 0.9198 0.9222 0.9242 0.9251 0.9261 0.9266 0.9276 0.9281 0.9295 0.931 0.9315 0.931 0.9315 0.931 0.9315 0.9315 0.9329 0.9329 0.9325 0.9344 0.9349 0.9349 0.9349 0.9349 0.9349 0.9349 0.9349 0.9349 0.9354 0.9369 0.9378 0.9383 0.9383 0.9393 0.9393 0.9393 0.9398 0.9408 0.9408 0.9412 0.9417 0.9422 0.9422 0.9427 0.9432 0.9432 0.9437 0.9432 0.9442 0.9442 0.9447 0.9452 0.9452 0.9452 0.9456 0.9461 0.9471 0.9471 0.9476 0.9476 0.9471 0.9471 0.9471 0.9476 0.9476 0.9476 0.9476 0.9476 0.9476 0.9476 0.9481 0.9481 0.9476 0.9476 0.9481 0.9481 0.9486 0.9486 0.9486 0.9486 0.9491 0.9491 0.9491 0.9491 0.9491 0.9491 0.9495 0.9495 0.95 0.95 0.9505 "
     ]
    }
   ],
   "source": [
    "# train for 100 epochs to test our approach\n",
    "lr = 1.\n",
    "params = weights, bias # puts the two tensors together in a tuple!\n",
    "\n",
    "for i in range(100):\n",
    "    train_epoch(linear_calc, lr, params)\n",
    "    print(epoch_accuracy(linear_calc), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PyTorch Modules\n",
    "Modules are classes that encapsulate the most common behavior we've implementer above. We can use nn.Linear for our simple linear model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(28*28,1)\n",
    "\n",
    "# optimizer will handle the changes in the parameters\n",
    "class BasicOptim:\n",
    "    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: p.data -= p.grad.data * self.lr\n",
    "\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params: p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance\n",
    "opt = BasicOptim(linear_model.parameters(), lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
