{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinan Yumurtaci\n",
    "# Code along for Chapter 4 of fast.ai's Practical Deep Learning for Coders\n",
    "\n",
    "# Digit classification task\n",
    "# Distinguish between images of digits\n",
    "# Data source: MNIST\n",
    "\n",
    "# 2nd section dedicated for using a NN on MNIST\n",
    "\n",
    "# 2021-03-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports as we follow along with fastbook\n",
    "\n",
    "from fastbook import *\n",
    "setup_book()\n",
    "from fastai.vision.widgets import *\n",
    "\n",
    "matplotlib.rc(\"image\", cmap = \"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6131 3s and 5421 5s.\n",
      "Loaded 6131 3s and 5421 5s!\n"
     ]
    }
   ],
   "source": [
    "# load the MNIST dataset\n",
    "# book loads only 3 and 7 for demonstration.\n",
    "\n",
    "# I'll be working on the classification for 3s and 5s\n",
    "# so that I learn new stuff!\n",
    "path = untar_data(URLs.MNIST)\n",
    "threes = (path/\"training\"/\"3\").ls().sorted()\n",
    "fives = (path/\"training\"/\"5\").ls().sorted()\n",
    "print(f\"There are {len(threes)} 3s and {len(fives)} 5s.\")\n",
    "\n",
    "# load the actual images into tensors\n",
    "three_tensors = [tensor(Image.open(f)) for f in threes]\n",
    "five_tensors = [tensor(Image.open(f)) for f in fives]\n",
    "\n",
    "print(f\"Loaded {len(three_tensors)} 3s and {len(five_tensors)} 5s!\")\n",
    "\n",
    "# stack the images for faster computation with PyTorch\n",
    "# also convert from [0, 255] to [0, 1]\n",
    "stacked_threes = torch.stack(three_tensors).float()/255\n",
    "stacked_fives = torch.stack(five_tensors).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put our training data in one variable\n",
    "train_x = torch.cat([stacked_threes, stacked_fives]).view(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11552, 784]), torch.Size([11552, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor of labels for our training data\n",
    "train_y = tensor([1]*len(threes) + [0] * len(fives)).unsqueeze(1)\n",
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip our data and labels into a dataset (a tuple of (x,y), as PyTorch requires)\n",
    "dset = list(zip(train_x, train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11552, 1902)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat the above steps for our test set\n",
    "test_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'testing'/'3').ls()]).float()/255\n",
    "test_5_tens = torch.stack([tensor(Image.open(o)) for o in (path/'testing'/'5').ls()]).float()/255\n",
    "\n",
    "test_x = torch.cat([test_3_tens, test_5_tens]).view(-1, 28*28)\n",
    "test_y = tensor([1]*len(test_3_tens) + [0] * len(test_5_tens)).unsqueeze(1)\n",
    "test_dset = list(zip(test_x, test_y))\n",
    "\n",
    "len(dset), len(test_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function that will return randomly initialized parameters of the specified size\n",
    "def init_params(size, std = 1.0):\n",
    "    return (torch.randn(size) * std).requires_grad_()\n",
    "\n",
    "weights = init_params((28*28,1))\n",
    "bias    = init_params(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6.2330],\n",
       "        [-10.6388],\n",
       "        [-20.8865],\n",
       "        ...,\n",
       "        [  0.7257],\n",
       "        [-10.1060],\n",
       "        [-18.4670]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create function that will calculate prediction using the input, and the weights and the bias\n",
    "def linear_calc(xb):\n",
    "    return xb @ weights + bias\n",
    "\n",
    "# calculate predictions for the training set using our initial, random parameters\n",
    "preds = linear_calc(train_x)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4904778301715851"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate our accuracy on the training set\n",
    "corrects = (preds > 0.5).float() == train_y # if prediction is stronger than 0.5, count as predicting for 3. else, 5\n",
    "corrects.float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Loss Function\n",
    "Simply accuracy won't work as a loss function, because it is not continious. If we were to change one of the parameters slightly, the accuracy would not change, as demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4904778301715851"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0] *= 1.00001\n",
    "preds = linear_calc(train_x)\n",
    "((preds>0.5).float() == train_y).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the Problem?\n",
    "The accuracy is the same after the small change, so our model won't have any way of knowing which small steps to take to better fit our training data. We need another metric that is continious and responds to small changes like above, so that we can deduce the next small step to take in the optimization loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a loss function for this specific MNIST task of distinguishing between 3s and 5s\n",
    "def mnist_loss(predictions, targets):\n",
    "    predictions = predictions.sigmoid() # sigmoid ensures that all values are between 0 and 1\n",
    "    # so that we can use the mean of distance from the target in the standardized units of 0 and 1.\n",
    "    return torch.where(targets == 1, 1-predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Note: Metrics vs Loss Functions\n",
    "Metrics is what we care about as humans. This is usually some form of accuracy; we care about how accurate models are.\n",
    "\n",
    "The machine learning model does not directly optimize for the metric. It insteads minimizes its loss function. This is easier because loss functions are designed to have meaningful gradients.\n",
    "\n",
    "Improving the loss function also tends to improve the metric simply because we design loss functions specifically for that use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "optimization_step:\n",
    "\n",
    "    pred = model(x)\n",
    "    loss = loss_func(pred, x)\n",
    "    loss.backward()\n",
    "    params -= parameters.grad * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our parameters\n",
    "weights = init_params((28*28, 1))\n",
    "bias = init_params(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataLoader from our dataset (zip of inputs and targets)\n",
    "dl = DataLoader(dset, batch_size=256)\n",
    "xb,yb = first(dl)\n",
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(test_dset, batch_size=256) # same for our testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sub-function that will do a forward and backward pass\n",
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    loss.backward()\n",
    "\n",
    "# function for one epoch (iteration over the dataset)\n",
    "def train_epoch(model, lr, params):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad*lr # using p.data rather than p prevents the calculation of the graident for this step\n",
    "            p.grad.zero_() # reset the gradient after each mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.655"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create functions for tracking the accuracy of our model\n",
    "def batch_accuracy(xb, yb):\n",
    "    preds = xb.sigmoid()\n",
    "    correct = (preds>0.5) == yb\n",
    "    return correct.float().mean()\n",
    "\n",
    "def epoch_accuracy(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb,yb in test_dl]\n",
    "    return round(torch.stack(accs).mean().item(), 4)\n",
    "\n",
    "epoch_accuracy(linear_calc) # this is our starting accuracy with random parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5396 0.5773 0.6227 0.6857 0.7253 0.757 0.7855 0.8104 0.8255 0.8426 0.8538 0.8607 0.87 0.8758 0.8792 0.8831 0.889 0.8944 0.8968 0.8997 0.9027 0.9046 0.908 0.91 0.9115 0.9134 0.9144 0.9159 0.9168 0.9183 0.9207 0.9227 0.9242 0.9266 0.9271 0.9276 0.9276 0.9286 0.929 0.9295 0.9295 0.9305 0.932 0.9325 0.9329 0.9329 0.9339 0.9339 0.9344 0.9349 0.9349 0.9354 0.9354 0.9359 0.9364 0.9369 0.9369 0.9369 0.9364 0.9373 0.9383 0.9393 0.9393 0.9393 0.9393 0.9398 0.9398 0.9403 0.9412 0.9412 0.9412 0.9417 0.9427 0.9427 0.9427 0.9427 0.9427 0.9427 0.9432 0.9427 0.9432 0.9442 0.9447 0.9447 0.9456 0.9456 0.9461 0.9471 0.9471 0.9471 0.9471 0.9471 0.9471 0.9471 0.9471 0.9471 0.9471 0.9476 0.9476 0.9476 "
     ]
    }
   ],
   "source": [
    "# train for 100 epochs to test our approach\n",
    "lr = 1.\n",
    "params = weights, bias # puts the two tensors together in a tuple!\n",
    "\n",
    "for i in range(100):\n",
    "    train_epoch(linear_calc, lr, params)\n",
    "    print(epoch_accuracy(linear_calc), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PyTorch Modules\n",
    "Modules are classes that encapsulate the most common behavior we've implementer above. We can use nn.Linear for our simple linear model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(28*28,1)\n",
    "\n",
    "# optimizer will handle the changes in the parameters\n",
    "class BasicOptim:\n",
    "    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: p.data -= p.grad.data * self.lr\n",
    "\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params: p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance\n",
    "opt = BasicOptim(linear_model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our training step can now be simplified:\n",
    "def train_epoch_module(model):\n",
    "    for bx, by in dl:\n",
    "        calc_grad(bx, by, model)\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a single function that will train the model for a given number of epochs\n",
    "def train_model(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch_module(model)\n",
    "        print(epoch_accuracy(model), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4932 0.4932 0.6441 0.5068 0.5146 0.563 0.626 0.6704 0.7036 0.7319 0.7593 0.7744 0.7871 0.8047 0.8154 0.8257 0.8359 0.8394 0.8423 0.8477 0.8545 0.8589 0.8643 0.8682 0.87 0.8729 0.8763 0.8802 0.8831 0.8861 0.8875 0.8919 0.8963 0.8993 0.9007 0.9012 0.9012 0.9017 0.9027 0.9041 0.9046 0.9071 0.908 0.909 0.9095 0.9105 0.9124 0.9139 0.9149 0.9168 0.9168 0.9173 0.9178 0.9193 0.9198 0.9203 0.9217 0.9217 0.9217 0.9222 0.9222 0.9232 0.9237 0.9237 0.9237 0.9242 0.9246 0.9251 0.9266 0.9266 0.9261 0.9271 0.9271 0.9271 0.9281 0.9286 0.9295 0.929 0.9295 0.9295 0.9295 0.9295 0.9295 0.9305 0.931 0.931 0.9315 0.932 0.932 0.9325 0.9329 0.9329 0.9339 0.9344 0.9344 0.9354 0.9359 0.9373 0.9373 0.9373 "
     ]
    }
   ],
   "source": [
    "train_model(linear_model, 100) # we should get nearly identical results (there is some random variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4932 0.4932 0.5524 0.5068 0.5146 0.5586 0.6211 0.668 0.7046 0.7319 0.7578 0.7754 0.7896 0.8008 0.813 0.8232 0.8374 0.8394 0.8428 0.8472 "
     ]
    }
   ],
   "source": [
    "# fast ai provides an SGD class, which can replace our BasicOptim class\n",
    "linear_model = nn.Linear(28*28,1)\n",
    "opt = SGD(linear_model.parameters(), lr)\n",
    "train_model(linear_model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast ai also has learners, which package all of this behavior into a convenient class\n",
    "dls = DataLoaders(dl, test_dl)\n",
    "learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n",
    "                loss_func=mnist_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.600523</td>\n",
       "      <td>0.468376</td>\n",
       "      <td>0.531020</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.594822</td>\n",
       "      <td>0.467399</td>\n",
       "      <td>0.531020</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.523141</td>\n",
       "      <td>0.516269</td>\n",
       "      <td>0.468980</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.252780</td>\n",
       "      <td>0.513305</td>\n",
       "      <td>0.471083</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.146622</td>\n",
       "      <td>0.465094</td>\n",
       "      <td>0.511041</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.093142</td>\n",
       "      <td>0.406516</td>\n",
       "      <td>0.575710</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.066462</td>\n",
       "      <td>0.357249</td>\n",
       "      <td>0.633018</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.053064</td>\n",
       "      <td>0.320736</td>\n",
       "      <td>0.675079</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.046333</td>\n",
       "      <td>0.292601</td>\n",
       "      <td>0.705047</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.042832</td>\n",
       "      <td>0.270470</td>\n",
       "      <td>0.732387</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(10, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What non-linearity Enables\n",
    "Our current model only maps inputs to outputs; it does not have an inner layer. if we were to add an inner layer without adding any non-linearity, we are not actually adding much sophistication to the model. To do that, we have functions that will add non-linearity. The simplest is a ReLU (Rectified Linear Unit), which is x = max(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a two-layer with NN with a ReLU in-between the two layers using the PyTorch modules\n",
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.479226</td>\n",
       "      <td>0.487757</td>\n",
       "      <td>0.468980</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.327245</td>\n",
       "      <td>0.505295</td>\n",
       "      <td>0.468980</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.222634</td>\n",
       "      <td>0.478486</td>\n",
       "      <td>0.471083</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.154948</td>\n",
       "      <td>0.422281</td>\n",
       "      <td>0.536278</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.113121</td>\n",
       "      <td>0.362004</td>\n",
       "      <td>0.616193</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.088611</td>\n",
       "      <td>0.312788</td>\n",
       "      <td>0.685594</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.074422</td>\n",
       "      <td>0.275570</td>\n",
       "      <td>0.733964</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.065979</td>\n",
       "      <td>0.247647</td>\n",
       "      <td>0.769716</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.060614</td>\n",
       "      <td>0.226268</td>\n",
       "      <td>0.789695</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.056921</td>\n",
       "      <td>0.209268</td>\n",
       "      <td>0.804942</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.054171</td>\n",
       "      <td>0.195380</td>\n",
       "      <td>0.815457</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.052004</td>\n",
       "      <td>0.183757</td>\n",
       "      <td>0.831230</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.050232</td>\n",
       "      <td>0.173831</td>\n",
       "      <td>0.839642</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.048744</td>\n",
       "      <td>0.165214</td>\n",
       "      <td>0.848580</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.047469</td>\n",
       "      <td>0.157647</td>\n",
       "      <td>0.853312</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.046358</td>\n",
       "      <td>0.150915</td>\n",
       "      <td>0.861724</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.045380</td>\n",
       "      <td>0.144894</td>\n",
       "      <td>0.865405</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.044512</td>\n",
       "      <td>0.139467</td>\n",
       "      <td>0.870662</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.043734</td>\n",
       "      <td>0.134578</td>\n",
       "      <td>0.873817</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.043033</td>\n",
       "      <td>0.130157</td>\n",
       "      <td>0.880652</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.042399</td>\n",
       "      <td>0.126153</td>\n",
       "      <td>0.885910</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.041823</td>\n",
       "      <td>0.122523</td>\n",
       "      <td>0.889590</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.041295</td>\n",
       "      <td>0.119223</td>\n",
       "      <td>0.891167</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.040808</td>\n",
       "      <td>0.116218</td>\n",
       "      <td>0.892744</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.040357</td>\n",
       "      <td>0.113455</td>\n",
       "      <td>0.895899</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.039937</td>\n",
       "      <td>0.110922</td>\n",
       "      <td>0.897476</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.039543</td>\n",
       "      <td>0.108586</td>\n",
       "      <td>0.899579</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.039175</td>\n",
       "      <td>0.106423</td>\n",
       "      <td>0.902734</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.038828</td>\n",
       "      <td>0.104423</td>\n",
       "      <td>0.904311</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.038502</td>\n",
       "      <td>0.102558</td>\n",
       "      <td>0.905889</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.038194</td>\n",
       "      <td>0.100816</td>\n",
       "      <td>0.908517</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.037904</td>\n",
       "      <td>0.099184</td>\n",
       "      <td>0.910095</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.037631</td>\n",
       "      <td>0.097650</td>\n",
       "      <td>0.910620</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.037373</td>\n",
       "      <td>0.096199</td>\n",
       "      <td>0.911672</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.037129</td>\n",
       "      <td>0.094814</td>\n",
       "      <td>0.912198</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>0.093515</td>\n",
       "      <td>0.914301</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.036681</td>\n",
       "      <td>0.092282</td>\n",
       "      <td>0.915878</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.036474</td>\n",
       "      <td>0.091087</td>\n",
       "      <td>0.916930</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.036277</td>\n",
       "      <td>0.089938</td>\n",
       "      <td>0.917981</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.036087</td>\n",
       "      <td>0.088861</td>\n",
       "      <td>0.919033</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(dls, simple_net, opt_func=SGD,\n",
    "                loss_func=mnist_loss, metrics=batch_accuracy)\n",
    "learn.fit(40, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9190325736999512"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD/CAYAAAD/qh1PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAH1dJREFUeJzt3Xl0nNWZ5/HvI5X2xZIs2caLbLzhYIINFg7BGEjIcsg0DQ1ZWJKQ5CQkZrLP9Azdc5imM326T3d6Op2TJunQIWFLCAmYBJoOoSdAsMEssh2TGGx5t+VNm7WvJT3zR5VMWSlZZaukt6r0+5xTR6q3bpUeX0k/vb7vvbfM3RERkcyVFXQBIiIysRT0IiIZTkEvIpLhFPQiIhlOQS8ikuEU9CIiGU5BLyKS4RT0IiIZTkEvIpLhQkEXAFBZWekLFiwIugwRkbSyefPmJnevGqtdSgT9ggULqK2tDboMEZG0YmYHEmmnoRsRkQynoBcRyXAKehGRDKegFxHJcAp6EZEMp6AXEclwCnoRkQyXEvPoRUQy0cDgEO09A3T0hqO3Adp7w7T3Dpy8v2p+OWuXjLnmaVwU9CIicbg73f2DtHT109YzcEpQd8QEdXtPmI6+yP2Rj/UODI35ddZdtUhBLyKSTO5Oa/cAh1t7ONzaw9HWHho7+2ju7Keps4/Gzn6aO/to6uwbM6jzc7Ioyc+hND908uPcsgJKCyL3S/JClEQfK8kPUVoQ/Ri9X5wXIpQ98SPoCnoRSSvhwaGTQyHtvQO09w7QOzBI38AQ/YND9A0M0Tc4RN/AIH3hIfrCQzR29HK4tZcjrT0cae2hu3/wlNfMzjIqinKpLM6jsjiXhZVFVBbnMr04j4qiXErzcygteDugh4M7ZxJCOhkU9CIyKdyd9p4wjdGz5eEz6KbOPk5090cCOjxEf3iIvvBgzOdDdPeHT4Z7z8Dg2F9shOlFucwpL2BxVTFXLKliTnkBc8rymVNWyDll+VQU5pKVZRPwr04NCnoRSZr+8BCHTnSzr7GL/c1d7G3qYl9jFweau2js7GNg0P/oOVkG0wpyKMjJJjeURV5o+GMWeTlZlOSHKMgtoCTv1LPp2KGQwtwQeaGsk88bfp28UBa52VkZHeKJSCjozawCuA/4ANAE/IW7/yROuzLg28A10UPfdfe7k1OqiAQlPDhES1f/KWPZw2flkTP0fg42d3HoRA+DQ2+HeUVRLudWFnHpounMLM0/OTQS+ZjH9OJcygtzyZ7iQTzREj2jvwfoB2YCK4GnzWybu28f0e5bQCGwAJgB/MbMDrj7j5JUr4gk2Ymuft461s6hlm6aOvtp7Ig3tDIQ97m5oSyqouG9fM40rl0xm3Mri07eygpzJ/lfI/GMGfRmVgTcCFzg7p3ARjN7EvgEcOeI5tcC17h7N7DfzO4DPgMo6EUC1hceZE9DFzuOtbPzWAdvHetg57F2jrf3ndKuOC908kLkwqoiVp9bceqZeMnbZ+MleSHMdDae6hI5o18KhN29LubYNuDKUdrbiM8vOMvaROQMdPWFOdrWQ/2JHo7EzDCpj3482tZ7clglNzuLxTOKWbOokmXnlHDerFIWVhZRVZJHfk52wP8SSbZEgr4YaB9xrA0oidP2GeBOM7uNyDDPZ4gM5fwRM7sduB2guro60XpFppzBIedgSzc7j3VwvL03Opzy9rDK8DBLvCmDs0rzmVNWQM38cuaWF3LerBKWzSphQWVR2kwNlPFLJOg7gdIRx0qBjjhtvwx8B9gFNAOPADfHe1F3vxe4F6CmpuaPL8WLTEEtXf3sONbOjqMd7DzWwY5j7dQd7zxlSmGWETPnO4/q6sKTn88uiwT77LICZpTkTcpiHEl9iQR9HRAysyXuvit6bAUw8kIs7t4C3Dp838z+FngtGYWKpKvu/jBNHcMzViJn480nz8Zjz8wjS+2HVRTlsmxWCTevrmbZrBLOm1XCnPICzVKRMzZm0Lt7l5mtB75hZp8lMuvmOuCykW3NbBHQGr19gMjQzGhj+SIZoXdgkDePtvPGoVbqGjr/aNbKyCGVYaX5oZNn4ufNKmFNcR7zygujY+YlVBXn6UKnJEWi0yvvAH4INBAZklnn7tvNbC3wK3cvjrZbBfwzUEbkfwK3xpmCKZK2woND7DzewRv1bbxR38ob9W3sPNZBOHqRs7wwh5ml+UwvzqW6uuzk7JTK4jyqYj6fXpxLXkgXPWVymHvww+M1NTVeW1sbdBkicR1t6+G5HQ08v6OBl3Y3nxwvL80PsWJeGRfOncaFc8tYMbeMWdPyA65WphIz2+zuNWO10xYIIiMMDjnb6lt5fkcDv3mrgTePRiadzS0v4CM1c1k1v5wVc8uYP71QQyuSFhT0IkQ23Hp1XwuPb67nuR0NNHf1k51lrJpfzp3XLOPqZTNYPKNYwS5pSUEvU9qxtl4e31LPz2sPsb+5m+K8EFe/YwbvXTaDK5dWaQm/ZAQFvUw5/eEhnttxnEdfP8Rv6xoZcnjXuRV8+eolXHPBORTk6iKpZBYFvWS8tp4B6o53sONoO28ebefZ7cdp7upnZmke665axEdWzWNBZVHQZYpMGAW9ZJT6E91sPnCCHceiK0uPtnOkrffk46X5IS5bVMnHLpnH2iWVWjkqU4KCXtJaeHCIrYda+c1bDTy34zh1xzsByMk2FlUVs/rcCs6bVcqycyJ7vMwqzdcFVZlyFPSSdtq6B3ihroHndjTw27pGWrsHCGUZq8+t4KM181izuJJFVcXkhnS2LgIKeklxA4ND1MWsRN12qI2dxzsYHHIqinK5etlM3rtsBmuXVlKanxN0uSIpSUEvKaWnf5Bn3zzG1oOtvFHfyvYj7fSFh4DI+4peOHca//Udi7hq2QxWzC3T5l4iCVDQS0po6xngoU37+dFL+2nu6qcgJ5sL5pTy8Uvnc+HcaVqJKjIOCnoJVENHL/dt3MePXzlIZ1+Y95xXxeevXETN/HLNiBFJEgW9BOJQSzfff3EPP6utJzw4xH+5cDbrrlzE+bNHvseNiIyXgl4mTUNHLy/vbub/vXWcX/3hGNlm3LhqDp+/YpEWLIlMIAW9TJiuvjCv7Wth4+4mNu5qYufxyLtPlhXm8OnLFvDZtQu1ra/IJFDQS1J19YV55LWDPLv9OFsOniA85OSGsli9oILrL5rD5YsrOX92qWbLiEwiBb0kRVdfmIdeOcC9L+6lpaufd86ZxueuWMjliytZNb+c/BxtFCYSFAW9jEt3f5iHNh3g+9GAv3JpFV993xIuqi4PujQRiVLQy1np7g/z8CsH+P5v99Lc1c/aJZV89X1LWTVfAS+SahT0ckY6+8L85NXIEE1TZz+XL67ka+9fwqr5FUGXJiKjUNBLQlq6+rn/pX08sOkAbT0DrFk8ne+9bymXLFDAi6Q6Bb2c1tG2Hv7txX088tpBegYGef/5M7njqkUagxdJIwp6iWtvYyf/+ts9PLH1MEMO162YzReuWsTSmSVBlyYiZ0hBL6dwd/7vs3Xc88JucrOzuOmSam6/YiHzKgqDLk1EzpKCXk7qDw9x5/o3WL/lMDdePJc7r1lGVUle0GWJyDgp6AWIzKZZ9/BmNuxq4uvvX8qX3rtYWwKLZAgFvdDQ3sunfvQ6O4938A8fvpCP1swLuiQRSSIF/RS3u6GT2374Gie6+/nBbTW857wZQZckIkmmoJ/Cave38NkHawllGT+9/VIunFsWdEkiMgEU9FPUM384xld+upXZZQU88OnVVE/XrBqRTKWgn2Laegb41n/W8cCm/aycV8Z9t11CRVFu0GWJyARS0E8RQ0POzzcf4h+e2cmJ7n5ufVc1/+tD51OQq+2DRTKdgn4K+N2hVv7ql39gW30bNfPLefC61SyfPS3oskRkkijoM1hTZx/ffGYnj9YeYkZJHv/8sZVct3K25seLTDEJBb2ZVQD3AR8AmoC/cPefxGmXB3wb+DMgB3gJ+IK7H05axTImd+fBTQf4x2d30tM/yOevWMiXrl5CcZ7+rotMRYn+5t8D9AMzgZXA02a2zd23j2j3FeDdwIVAG3Av8B3ghuSUK2PpCw/yPx57g1/+7ghrl1TyV9cuZ/GM4qDLEpEAjRn0ZlYE3Ahc4O6dwEYzexL4BHDniObnAr929+PR5z4K/FNyS5bRtHT18/mHanl9/wn+/IPnccdVizRMIyIJndEvBcLuXhdzbBtwZZy29wHfNrPZQCtwK/CrcVcpY9rb2Mln7n+dI229fOfmi7h2xeygSxKRFJFI0BcD7SOOtQHxNibfBRwCDgODwO+BL8Z7UTO7HbgdoLq6OsFyJZ5X9zbz+Yc3k2XGI5+7VO/bKiKnyEqgTSdQOuJYKdARp+09QB4wHSgC1jPKGb273+vuNe5eU1VVlXjFcoonttbz8fteZXpRLr+4Y41CXkT+SCJBXweEzGxJzLEVwMgLsRC5UHu/u7e4ex+RC7Grzaxy/KVKLHfnW/9Zx9ce3UbN/ArWr1ujbQxEJK4xg97du4icmX/DzIrMbA1wHfBQnOavA580s2lmlgPcARxx96ZkFj3VDQ05f/7YG3z7N7v48Kq5PPCZ1UwrzAm6LBFJUYmc0UMksAuABuARYJ27bzeztWbWGdPuvwO9RMbqG4EPEZlTL0n0t//xFo9trucrVy/hmx++kNxQot9GEZmKEppH7+4twPVxjm8gcrF2+H4zkZk2MkF+sGEvP9i4j09dtoCvvm+Jpk+KyJh0KphGntp2hL95+i0+9M5Z3PUn5yvkRSQhCvo0sWlPM//tZ9tYvaCCf/roSrKzFPIikhgFfRrYcayd2x+qZf70Qv7tkzXk52hrYRFJnII+xR1p7eFTP3ydwtxs7tfsGhE5Cwr6FNbWM8CnfvQaXX1h7v/0auaUFQRdkoikIe1bm6J6Bwa5/cFa9jV18cCnV/OOc0YuThYRSYyCPkX99VNv8uq+Fr5900ouW6yFxSJy9jR0k4L2NXXx6OsH+fSaBVy3ck7Q5YhImlPQp6B/eW43OdlZrLtqUdCliEgGUNCnmP1NXfzid4e59V3zmVGSH3Q5IpIBFPQp5l+e300oy/jClQuDLkVEMoSCPoUcaO7iia2HueVd1cwo1dm8iCSHgj6F3PP8brKzjC9cqbF5EUkeBX2KONjczeNbDnPL6mpm6mxeRJJIQZ8idDYvIhNFQZ8CDrV08/iWem6+ZB6zpulsXkSSS0GfAr77wm6yzPiC5s2LyARQ0Aes/kQ3P6+t52OXzOOcadq0TESST0EfsHue34MZWgUrIhNGQR+gw609PLb5EB+7ZB6ztQWxiEwQBX2Avvv8bgDWXbU44EpEJJMp6ANypLWHn9Ue4iM18/SGIiIyoRT0Abn/5f24wx0amxeRCaagD0B4cIgnth7mvctmMLe8MOhyRCTDKegDsHF3E40dfdxw8dygSxGRKUBBH4D1Ww5TVpjDe5ZVBV2KiEwBCvpJ1tE7wK+3H+PaC2eTF8oOuhwRmQIU9JPsV78/Rl94iBsu1nvBisjkUNBPsse31LOwsoiV88qCLkVEpggF/SQ61NLNq/tauOHiOZhZ0OWIyBShoJ9ET2w9DMD1F2nYRkQmj4J+krg767fUc+nCCs2dF5FJpaCfJFsOtrK/uVtz50Vk0inoJ8n6LfXk52RxzQWzgi5FRKaYhILezCrM7Akz6zKzA2Z2yyjtfmVmnTG3fjP7fXJLTj994UGe2naEDy6fRUl+TtDliMgUE0qw3T1APzATWAk8bWbb3H17bCN3vyb2vpm9ADyXhDrT2nNvNdDeG9awjYgEYswzejMrAm4E7nL3TnffCDwJfGKM5y0A1gIPjr/M9Pb4lsPMKMljzaLpQZciIlNQIkM3S4Gwu9fFHNsGLB/jeZ8ENrj7/ngPmtntZlZrZrWNjY0JFZuOmjv7eGFnA9dfNIdQti6JiMjkSyR5ioH2EcfagJIxnvdJ4P7RHnT3e929xt1rqqoyd3Ovp7YdITzk3KhhGxEJSCJB3wmUjjhWCnSM9gQzuxyYBTx29qVlhvVbD7N8dinnzRrr76KIyMRIJOjrgJCZLYk5tgLYPkp7gNuA9e7eOZ7i0t2u4x28Ud+mi7AiEqgxg97du4D1wDfMrMjM1gDXAQ/Fa29mBcBHOc2wzVSxfuthsrOMP10xO+hSRGQKS/Tq4B1AAdAAPAKsc/ftZrbWzEaetV8PtALPJ6/M9DM45Pxi62GuXFpFVUle0OWIyBSW0Dx6d28hEuAjj28gcrE29tgjRP4YTGmv7WvhaFsvf/mhdwRdiohMcZrvN0E27GoklGW8d9mMoEsRkSlOQT9BXt7TzIp5ZRTlJbr4WERkYijoJ0BH7wC/P9zGuxdqJayIBE9BPwFe39/C4JBzmbY8EJEUoKCfAJv2NJMbyuLi+eVBlyIioqCfCC/vaebi6jLyc7KDLkVEREGfbK3d/bx5tJ3LFlUGXYqICKCgT7pX9rbgDu/W+LyIpAgFfZJt2tNEQU42K+aWBV2KiAigoE+6TXubqVlQTm5IXSsiqUFplESNHX3UHe/U+LyIpBQFfRK9srcZ0Pi8iKQWBX0SvbynmZK8EBfMHvk+LSIiwVHQJ9Ere5tZfW6F3htWRFKKEilJjrb1sK+pS8M2IpJyFPRJsmlPZHxeF2JFJNUo6JPk5T3NlBfmsExvAi4iKUZBnwTuzqY9zVy6cDpZWRZ0OSIip1DQJ8Ghlh4Ot/ZofF5EUpKCPgk27W0C0P7zIpKSFPRJ8PKeZqpK8lhUVTx2YxGRSaagH6fh8fl3L5yOmcbnRST1KOjHaU9jFw0dfRqfF5GUpaAfp017h+fPK+hFJDUp6Mdp054m5pQVUF1RGHQpIiJxKejHYWjIeWVvC5dqfF5EUpiCfhx2Hu+gpatfwzYiktIU9OPw8h7tPy8iqU9BPw6b9jSzYHohs8sKgi5FRGRUCvqzNDjkvLqvWWfzIpLyFPRn6a2j7XT0hrl0oYJeRFKbgv4sbT14AoCLq8sDrkRE5PQU9Gdpy8FWqkrymFuu8XkRSW0K+rO05eAJLq4u0/x5EUl5CQW9mVWY2RNm1mVmB8zsltO0vdjMXjSzTjM7bmZfSV65qaG5s48Dzd0athGRtBBKsN09QD8wE1gJPG1m29x9e2wjM6sEngG+BjwG5AJzk1duath6sBWAi+cr6EUk9Y15Rm9mRcCNwF3u3unuG4EngU/Eaf514Nfu/mN373P3Dnd/K7klB2/LwROEsox3zpkWdCkiImNKZOhmKRB297qYY9uA5XHaXgq0mNnLZtZgZk+ZWXW8FzWz282s1sxqGxsbz7zyAG05eILzZ5eSn5MddCkiImNKJOiLgfYRx9qAkjht5wK3AV8BqoF9wCPxXtTd73X3GnevqaqqSrzigIUHh9h2qE3j8yKSNhIZo+8ESkccKwU64rTtAZ5w99cBzOyvgSYzm+bubeOqNEXsONZBz8AgF1WXBV2KiEhCEjmjrwNCZrYk5tgKYHuctm8AHnPf47RJa1ooJSLpZsygd/cuYD3wDTMrMrM1wHXAQ3Ga/wj4MzNbaWY5wF3Axkw5mwctlBKR9JPogqk7gAKggciY+zp3325ma82sc7iRuz8H/CXwdLTtYmDUOffpSAulRCTdJDSP3t1bgOvjHN9A5GJt7LHvAd9LSnUppim6UOqW1XEnEomIpCRtgXAGtFBKRNKRgv4MaKGUiKQjBf0Z2HLgBMu1UEpE0oyCPkHhwSHeqG/jIk2rFJE0o6BPkBZKiUi6UtAnSAulRCRdKegTpIVSIpKuFPQJ0kIpEUlXCvoENOkdpUQkjSnoE6CFUiKSzhT0CdBCKRFJZwr6BGihlIikMwX9GLRQSkTSnYJ+DMMLpTQ+LyLpSkE/hi3RhVIXzdOKWBFJTwr6MWw5cEILpUQkrSnox7DlYKsWSolIWlPQn0ZTZx8HW7RQSkTSm4L+NLYciG5kpguxIpLGFPSnseVgqxZKiUjaU9CfxtaDWiglIulPQT+K3oFBttW3athGRNKegn4Umw+coHdgiMsXVwZdiojIuCjoR/HirkZyso1LF04PuhQRkXFR0I9i464mLq4upygvFHQpIiLjoqCPo6mzj+1H2rliaVXQpYiIjJuCPo6XdjcBaHxeRDKCgj6OF+uaKCvM4QLNnxeRDKCgH8Hd2bi7kTWLK8nO0v42IpL+FPQj7Gro5Hh7H1cs0bCNiGQGBf0IL9Y1AnD5El2IFZHMoKAfYcOuJhZWFTGnTPvPi0hmUNDH6AsP8uq+Zq7Q2byIZBAFfYzN+yPbHqzV+LyIZJCEgt7MKszsCTPrMrMDZnbLKO3uNrMBM+uMuS1MbskT58VdTdr2QEQyTqLr++8B+oGZwErgaTPb5u7b47R91N0/nqwCJ9OGXY1cpG0PRCTDjHlGb2ZFwI3AXe7e6e4bgSeBT0x0cZOpeXjbAw3biEiGSWToZikQdve6mGPbgOWjtL/WzFrMbLuZrRvtRc3sdjOrNbPaxsbGMyh5YmyMbnuwVhdiRSTDJBL0xUD7iGNtQEmctj8D3gFUAZ8D/reZ3RzvRd39Xnevcfeaqqrgw3XDLm17ICKZKZGg7wRKRxwrBTpGNnT3N939iLsPuvvLwLeBD4+/zInl7mzY1ciaRdr2QEQyTyJBXweEzGxJzLEVQLwLsSM5kPLJuTu67YGmVYpIJhoz6N29C1gPfMPMisxsDXAd8NDItmZ2nZmVW8Rq4MvAL5NddLK9uCu6LbGCXkQyUKILpu4ACoAG4BFgnbtvN7O1ZtYZ0+4mYDeRYZ0Hgb939weSWfBE2LCrkYVVRcwtLwy6FBGRpEtowri7twDXxzm+gcjF2uH7cS+8prK+8CCv7G3mpkuqgy5FRGRCTPktEDYfiGx7oHeTEpFMNeWDfsOuJkJZxqWLtO2BiGQmBf2uRi6eX06xtj0QkQw1pYNe2x6IyFQwpYP+pT3NuOvdpEQks02J8YrBIaeho5cjrT3Un+jhSGvk85f2NDGtIId3atsDEclgaR30v61r5G/+/c3TtunuH+R4ey/hIT/leFlhDrOnFfCl9y7WtgciktHSOuiL80IsmVl82jZ5oWxml+Uzp6ww+rGAc8oKdPFVRKaMtE67VfPLWTV/VdBliIiktCl9MVZEZCpQ0IuIZDgFvYhIhlPQi4hkOAW9iEiGU9CLiGQ4Bb2ISIZT0IuIZDhz97FbTXQRZo3AgbN8eiXQlMRykkm1nZ1Urg1Suz7VdnbStbb57j7mrowpEfTjYWa17l4TdB3xqLazk8q1QWrXp9rOTqbXpqEbEZEMp6AXEclwmRD09wZdwGmotrOTyrVBaten2s5ORteW9mP0IiJyeplwRi8iIqehoBcRyXBpG/RmVmFmT5hZl5kdMLNbgq4plpm9YGa9ZtYZve0MqI4vmlmtmfWZ2f0jHrvazHaYWbeZPW9m81OhNjNbYGYe03edZnbXJNeWZ2b3RX+2Oszsd2Z2TczjgfXd6WpLkb572MyOmlm7mdWZ2WdjHgv6Zy5ubanQbzE1Lolmx8Mxx26Jfr+7zOwXZlZxRi/q7ml5Ax4BHgWKgcuBNmB50HXF1PcC8NkUqOMG4Hrge8D9Mccro332ESAf+CbwSorUtgBwIBRgvxUBd0dryQL+BOiI3g+078aoLRX6bjmQF/18GXAMWBV0v41RW+D9FlPjs8AG4OGYmjuAK6J59xPgp2fymmn5VoJmVgTcCFzg7p3ARjN7EvgEcGegxaUYd18PYGY1wNyYh24Atrv7z6OP3w00mdkyd98RcG2Bc/cuImE67N/NbB+RUJhOgH03Rm2bJ/rrj8Xdt8fejd4WEakv6J+50WprnoyvPxYzuwloBV4GFkcP3wo85e4vRtvcBbxlZiXu3pHI66br0M1SIOzudTHHthH5y5dK/s7MmszsJTO7KuhiRlhOpM+Ak+Gxh9TqwwNmVm9mPzKzyiALMbOZRH7utpNifTeitmGB9p2ZfdfMuoEdwFHgP0iRfhultmGB9ZuZlQLfAL4+4qGR/bYH6CfyPU9IugZ9MdA+4lgbUBJALaP5n8BCYA6RebBPmdmiYEs6RTGRPouVKn3YBFwCzCdyFlgC/DioYswsJ/r1H4ieeaZM38WpLSX6zt3viH7ttcB6oI8U6bdRakuFfvs/wH3uXj/i+Lj7LV2DvhMoHXGslMg4Vkpw91fdvcPd+9z9AeAl4ENB1xUjZfvQ3Tvdvdbdw+5+HPgi8AEzCyJIs4CHiJxBfTF6OCX6Ll5tqdR37j7o7huJDMutI0X6LV5tQfebma0E3gd8K87D4+63tByjB+qAkJktcfdd0WMrOPW/rqnGAQu6iBjbgduG70SveywiNftweFXfpJ6YmJkB9wEzgQ+5+0D0ocD77jS1jRRI340Q4u3+SbWfueHaRprsfruKyAXhg5FvLcVAtpmdDzxDJN8AMLOFQB6RHExM0FeYx3Fl+qdEZt4UAWtIoVk3QBnwQSIzC0JELqZ0AUsDqCUUrePviJz9DddUFe2zG6PH/p7JnwExWm3vAs4j8ks2ncjsqucD6Lt/BV4BikccT4W+G622QPsOmAHcNBxU0d+DLuBPg+63MWoLut8KgVkxt38EHov22XIiQ9Vro3n3MGc462bSfjAnoGMqgF9Ev1EHgVuCrimmtirgdSL/tWqN/kK+P6Ba7ubt2QXDt7ujj72PyAWpHiLTQRekQm3AzcC+6Pf2KPAgMGuSa5sfraeXyH+dh2+3Bt13p6st6L6L/uz/Nvpz3w78HvhczONB9tuotQXdb3FqvZvo9Mro/VuiOdcF/BKoOJPX0143IiIZLl0vxoqISIIU9CIiGU5BLyKS4RT0IiIZTkEvIpLhFPQiIhlOQS8ikuEU9CIiGU5BLyKS4f4/ttY6KL30idQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the training progress is recorded in *recorder*\n",
    "\n",
    "# plot accuracy over epochs\n",
    "plt.plot(L(learn.recorder.values).itemgot(2));\n",
    "\n",
    "# and observe the final accuracy\n",
    "learn.recorder.values[-1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going Deeper\n",
    "ADD MOAR LAYERS!\n",
    "\n",
    "(even better)\n",
    "\n",
    "STEAL SOME LAYERS (Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-7475073293b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataLoaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/fastai/vision/data.py\u001b[0m in \u001b[0;36mfrom_folder\u001b[0;34m(cls, path, train, valid, valid_pct, seed, vocab, item_tfms, batch_tfms, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m                            \u001b[0mitem_tfms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitem_tfms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                            batch_tfms=batch_tfms)\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/fastai/data/core.py\u001b[0m in \u001b[0;36mfrom_dblock\u001b[0;34m(cls, dblock, source, path, bs, val_bs, shuffle, device, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_dblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_bs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_bs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_bs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     _docs=dict(__getitem__=\"Retrieve `DataLoader` at `i` (`0` is training, `1` is validation)\",\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/fastai/data/block.py\u001b[0m in \u001b[0;36mdataloaders\u001b[0;34m(self, source, path, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mdsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'verbose'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter_item\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_tfms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_tfms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/fastai/data/block.py\u001b[0m in \u001b[0;36mdatasets\u001b[0;34m(self, source, verbose)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitter\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mRandomSplitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mpv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{len(splits)} datasets of sizes {','.join([str(len(s)) for s in splits])}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDatasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_type_tfms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_inp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/fastai/data/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, items, tfms, tls, n_inp, dl_type, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_inp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtls\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtls\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTfmdLists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/fastai/data/core.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_inp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtls\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtls\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTfmdLists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/fastcore/foundation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/fastai/data/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, items, tfms, use_list, do_setup, split_idx, train_setup, splits, types, verbose, dl_type)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_setup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mpv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Setting up {self.tfms}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_setup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_setup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/fastai/data/core.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, train_setup)\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'  - {t}'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "dls = ImageDataLoaders.from_folder(path)\n",
    "learn = cnn_learner(dls, resnet18, pretrained=False,\n",
    "                    loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
